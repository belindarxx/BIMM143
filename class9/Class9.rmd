---
title: "Class 9"
author: "Belinda Xue"
date: "10/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Input
```{r}
# read the data
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df)
# use head() to only read the first 6 rows
```

Notes that the 'id' and 'diagnosis' columns will not be used for most of the following steps, and the last "x" column in the following analysis

We have `r nrow(wisc.df)` samples in this dataset
# fill in the blank for you if r xxx

How many patients we had?
```{r}
nrow(wisc.df)
```


How many benign (not cancerous) and and malignant (cancerous)) samples do we have in the dataset? 
```{r}
table(wisc.df$diagnosis)
```


Exclude the "id", "diagnosis", "x" columns! 
```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])


# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id

head(wisc.data)
```


Store the diagnosis for reference in the future as a separate vector
- Finally, setup a separate new vector called diagnosis that contains the data from the diagnosis column of the origional dataset. We will use this later to check our results.
```{r}
# Create diagnosis vector for later
diagnosis <- wisc.df$diagnosis
```



## Exploratory data analysis

Q1. How many observations are in this dataset?
Q2. How many of the observations have a malignant diagnosis?
Q3. How many variables/features in the data are suffixed with _mean?

HINT: The functions dim(), nrow(), table(), length() and grep() may be useful for answering the first 3 questions above.


Q1. How many observations are in this dataset?
```{r}
dim(wisc.df)
# how many rows and how many columns 
# total 33 different tests are done with 569 patients
```

Q2. How many of the observations have a malignant diagnosis?
```{r}
table(wisc.df$diagnosis)
# in the diagnosis column, how many malignant cancer?
# total 212 malignant 
```

Q3. How many variables/features in the data are suffixed with _mean?
```{r}
colnames(wisc.df)
# give you all vectors of the column names
```

```{r}
grep("_mean", colnames(wisc.df))
# within the column names, find the patter "_mean"
```

```{r}
grep("_mean", colnames(wisc.df), value= TRUE)
# within the column names, find the patter "_mean"
# use value=TRUE --> will spit out the name instead of the column number
```

```{r}
length(grep("_mean", colnames(wisc.df)))
# tell you how many as a number instead of giving you invidivual  names of the column that have _mean
```


## 2. Principal Compoent Analysis

###Performing PCA

The next step in your analysis is to perform principal component analysis (PCA) on wisc.data.

It is important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data include:
- The input variables use different units of measurement.
- The input variables have significantly different variances.

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like you’ve done before.


```{r}
# Check column means and standard deviations to see if we need to use scaling

# use round() to round the mean and sd

round (colMeans(wisc.data), 3)
# they all look different / mean and SD not silimar ==> need to set scale = TRUE

```

```{r}
round ( apply(wisc.data,2,sd), 3) 
# they all look different / mean and SD not silimar ==> need to set scale = TRUE
```


These values look very different so I will use `scale=TRUE` when I run PCA
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```
# can look at the culmulative proportion to see the overall variant
- "proportion of variance" is the what each PC has 



Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
- 44.27% in PC1
```{r}
x <- summary(wisc.pr)
x$importance[,1]
```


Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
- 3 PC needed; PC1 + PC2 + PC3 = 72.5%

```{r}
which(x$importance["Cumulative Proportion",] > 0.7) [1]
```


Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
- 7 ; PC1 to PC7 = 91% variant
```{r}
which(x$importance["Cumulative Proportion",] > 0.9) [1]
```


Scree-plot that shows what each PC have how many variant
```{r}
plot(wisc.pr)
```


Lets make a plot of PC1 and PC2

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2])
```

Color by cancer/ non-cancer... 
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis, xlab="PC1", ylab="PC2")
# color diagnosis, either benign and malignant
# but which color represent benign and malgant? 
```


##Interpreting PCA results

Now you will use some visualizations to better understand your PCA model. A common visualization for PCA results is the so-called biplot.

However, you will often run into some common challenges with using biplots on real-world data containing a non-trivial number of observations and variables. Here we will need to look at some alternative visualizations. You are encouraged to experiment with additional visualizations before moving on to the next section

Create a biplot of the wisc.pr using the biplot() function.

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
```{r}
biplot(wisc.pr)
```


Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, 1], wisc.pr$x[, 3] , col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

```

Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.

Overall, the plots indicate that principal component 1 is capturing a separation of malignant from benign samples. This is an important and interesting result worthy of further exploration - as we will do in the next sections! 


### Variance explained

In this exercise, you will produce scree plots showing the proportion of variance explained as the number of principal components increases.


As you look at these plots, ask yourself if there’s an ‘elbow’ in the amount of variance explained that might lead you to pick a natural number of principal components. If an obvious elbow does not exist, as is typical in some real-world datasets, consider how else you might determine the number of principal components to retain based on the scree plot.


Calculate the variance of each principal component by squaring the sdev component of wisc.pr (i.e. wisc.pr$sdev^2). Save the result as an object called pr.var.


```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
# variance in each PC 
# 13.28 is the variance 
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.

```{r}
# Variance explained by each principal component: pve
pve <-  pr.var / sum(pr.var) 

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```




...









### Communicating PCA results

In this section we will check your understanding of the PCA results, in particular the loadings and variance explained. The loadings, represented as vectors, explain the mapping from the original features to the principal components. The principal components are naturally ordered from the most variance explained to the least variance explained.


Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
# what is angle rotation relative to the regualr xy axis
```



Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
x <- summary(wisc.pr)
which(x$importance["Cumulative Proportion",] > 0.8) [1]
```

.....

#3. Hierarchical clustering

Hierarchical clustering of case data
The goal of this section is to do hierarchical clustering of the observations. Recall from our last class that this type of clustering does not assume in advance the number of natural groups that exist in the data.

As part of the preparation for hierarchical clustering, the distance between all pairs of observations are computed. Furthermore, there are different ways to link clusters together, with single, complete, and average being the most common linkage methods.


Scale the wisc.data data and assign the result to data.scaled.
```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
data.scaled
```


```{r}
data.dist <- dist(data.scaled)
```


Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```




Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(wisc.hclust, col="red", lty=2)
```



Selecting number of clusters
In this section, you will compare the outputs from your hierarchical clustering model to the actual diagnoses. Normally when performing unsupervised learning like this, a target variable (i.e. known answer or labels) isn’t available. We do have it with this dataset, however, so it can be used to check the performance of the clustering model.

When performing supervised learning - that is, when you’re trying to predict some target variable of interest and that target variable is available in the original data - using clustering to create new features may or may not improve the performance of the final model.

This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature.

Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.

..........

skip k-means

........

## 5. Combining methods


Clustering on PCA results
In this final section, you will put together several steps you used earlier and, in doing so, you will experience some of the creativity and open endedness that is typical in unsupervised learning.

Recall from earlier sections that the PCA model required significantly fewer features to describe 70%, 80% and 95% of the variability of the data. In addition to normalizing data and potentially avoiding over-fitting, PCA also uncorrelates the variables, sometimes improving the performance of other modeling techniques.

Let’s see if PCA improves or degrades the performance of hierarchical clustering.

Original data
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), "ward.D2")
```


Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

???


```{r}
plot(wisc.pr.hclust)
```

cut tree at k=2
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
# 216 in group 1, 353 in group 2
```

```{r}
table(grps, diagnosis)
# group 1 include 28 benign, 188 diagnosis as maligant --> group 1 mostly maligant.. 28 ppl false positive 
# group 2 include 329 benign, 24 diagnosis as maligant --> 24 false negative?? misdiagnose, 
# trade off of sensitivity specificity ????
# group 1 sensitivty cuz they correctly identify the ill patients
# group 2 specoficity cuz they correctly reject the health patient as healthy
```



Re-order the color so group 2 comes first so group2 = red, group 1= black ----> group2 = benign? group1= malignant
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

...


hclust(dist(x))
x can be wisc.pr$[,1] or wisc.pr$[, 1:3] 
cluster results..??? 

.......


# Section 7: Prediction


We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

Plot these new samples on our PC1 VS PC2 plot.... 
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

Q17. Which of these new patients should we prioritize for follow up based on your results?

```{r}
proj(wisc.pr)
```









